[Main]

# The host of the Datadog intake server to send Agent data to
dd_url: https://app.datadoghq.com

# The Datadog api key to associate your Agent's data with your organization.
# Can be found here:
# https://app.datadoghq.com/account/settings
api_key: APIKEYHERE

# If you need a proxy to connect to the Internet, provide the settings here
# proxy_host: my-proxy.com
# proxy_port: 3128
# proxy_user: user
# proxy_password: password
# To be used with some proxys that return a 302 which make curl switch from POST to GET
# See http://stackoverflow.com/questions/8156073/curl-violate-rfc-2616-10-3-2-and-switch-from-post-to-get
# proxy_forbid_method_switch: no

# If you run the agent behind haproxy, you might want to set this to yes
# skip_ssl_validation: no

# Force the hostname to whatever you want.
#hostname: mymachine.mydomain

# Set the host's tags
#tags: mytag0, mytag1

# Change port the Agent is listening to
# listen_port: 17123

# Start a graphite listener on this port
# graphite_listen_port: 17124

# Certificate file.
# ca_certs = datadog-cert.pem

# Collect instance metadata
# The Agent will try to collect instance metadata for EC2 and GCE instances by
# trying to connect to the local endpoint: http://169.254.169.254
# See http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AESDG-chapter-instancedata.html
# and https://developers.google.com/compute/docs/metadata
# for more information
# collect_instance_metadata: yes

# If enabled the collector will capture a metric for check run times.
# check_timings: no

# ========================================================================== #
# DogStatsd configuration                                                    #
# ========================================================================== #

# DogStatsd is a small server that aggregates your custom app metrics. For
# usage information, check out http://api.datadoghq.com

#  Make sure your client is sending to the same port.
# dogstatsd_port : 8125

# By default dogstatsd will post aggregate metrics to the Agent (which handles
# errors/timeouts/retries/etc). To send directly to the datadog api, set this
# to https://app.datadoghq.com.
# dogstatsd_target : http://localhost:17123

# ========================================================================== #
# Service-specific configuration                                             #
# ========================================================================== #

# -------------------------------------------------------------------------- #
#   Disk                                                                     #
# -------------------------------------------------------------------------- #

# To filter out a pattern of devices from collection, configure a regex here to
# match the disks to ignore
# DEPRECATED: use conf.d/disk.yaml instead to configure it
#device_blacklist_re: D:

# -------------------------------------------------------------------------- #
#   Ganglia                                                                  #
# -------------------------------------------------------------------------- #

# Ganglia host where gmetad is running
#ganglia_host: localhost

# Ganglia port where gmetad is running
#ganglia_port: 8651

# -------------------------------------------------------------------------- #
#   Cassandra                                                                #
# -------------------------------------------------------------------------- #
#cassandra_host: localhost
#cassandra_port: 8080
#cassandra_nodetool: /usr/bin/nodetool

# -------------------------------------------------------------------------- #
#  Dogstream (log file parser)
# -------------------------------------------------------------------------- #

# Comma-separated list of logs to parse and optionally custom parsers to use.
# The form should look like this:
#
#   dogstreams: C:\path\to\log1:parsers_module:custom_parser, C:\path\to\log2, C:\path\to\log3, ...
#
# Or this:
#
#   dogstreams: C:\path\to\log1:C:\path\to\my\parsers_module.py:custom_parser, C:\path\to\log2, C:\path\to\log3, ...
#
# Each entry is a path to a log file and optionally a Python module/function pair
# separated by colons.
#
# Custom parsers should take a 2 parameters, a logger object and
# a string parameter of the current line to parse. It should return a tuple of
# the form:
#   (metric (str), timestamp (unix timestamp), value (float), attributes (dict))
# where attributes should at least contain the key 'metric_type', specifying
# whether the given metric is a 'counter' or 'gauge'.
#
# Unless parsers are specified with an absolute path, the modules must exist in
# the Agent's PYTHONPATH. You can set this as an environment variable when
# starting the Agent. If the name of the custom parser function is not passed,
# 'parser' is assumed.
#
# If this value isn't specified, the default parser assumes this log format:
#     metric timestamp value key0=val0 key1=val1 ...
#

# ========================================================================== #
# Custom Emitters                                                            #
# ========================================================================== #

# Comma-separated list of emitters to be used in addition to the standard one
#
# Expected to be passed as a comma-separated list of colon-delimited
# name/object pairs.
#
# custom_emitters: /usr/local/my-code/emitters/rabbitmq.py:RabbitMQEmitter
#
# If the name of the emitter function is not specified, 'emitter' is assumed.


# ========================================================================== #
# Custom Checks
# ========================================================================== #

# Comma-separated list of additional metric checks
#
# Expected to be passed as a comma-separated list of colon-delimited
# name/object pairs.
#
# custom_checks: /usr/local/my-code/checks/foo.py:FooCheck
#
# If the name of the check is not specified, 'Check' is assumed.


# ========================================================================== #
# Logging
# ========================================================================== #

# log_level: INFO
log_to_event_viewer: no
