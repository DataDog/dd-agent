[Main]

# The host of the Datadog intake server to send agent data to
dd_url: https://app.datadoghq.com

# The Datadog api key to associate your agent's data with your organization.
# Can be found here:
# https://app.datadoghq.com/account/settings
api_key: APIKEYHERE

# Boolean to enable debug_mode, which outputs massive amounts of log messages
# to the /tmp/ directory.
debug_mode: no

# Force the hostname to whatever you want.
#hostname: mymachine.mydomain

# Set the host's tags
#tags: mytag0, mytag1

# Use the amazon EC2 instance-id instead of hostname (unless hostname is
# explicitly set)
use_ec2_instance_id: yes

# Use mount points instead of volumes to track disk and fs metrics
use_mount: no

# Change port the agent is listening to
# listen_port: 17123

# Start a graphite listener on this port
# graphite_listen_port: 17124

# ========================================================================== #
# Pup configuration
# ========================================================================== #

# Pup is a small server that displays metric data collected by the agent.
# Think of it as a fancy status page or a toe dip into the world of
# datadog. It can be connected to on the port below.

# use_pup: yes
# pup_port: 17125
# pup_url: http://localhost:17125

# ========================================================================== #
# DogStatsd configuration                                                    #
# ========================================================================== #

# DogStatsd is a small server that aggregates your custom app metrics. For
# usage information, check out http://api.datadoghq.com

#  Make sure your client is sending to the same port.
# dogstatsd_port : 8125

# By default dogstatsd will post aggregate metrics to the agent (which handles
# errors/timeouts/retries/etc). To send directly to the datadog api, set this
# to https://app.datadoghq.com.
# dogstatsd_target : http://localhost:17123

## The dogstatsd flush period.
# dogstatsd_interval : 10


# ========================================================================== #
# Service-specific configuration                                             #
# ========================================================================== #

# -------------------------------------------------------------------------- #
#   Apache                                                                   #
# -------------------------------------------------------------------------- #

# Url to Apache's status page. Must have mod_status installed.
# See http://httpd.apache.org/docs/2.0/mod/mod_status.html for details.
#apache_status_url: http://www.example.com/server-status/?auto


# -------------------------------------------------------------------------- #
#   Ganglia                                                                  #
# -------------------------------------------------------------------------- #

# Ganglia host where gmetad is running
#ganglia_host: localhost

# Ganglia port where gmetad is running
#ganglia_port: 8651


# -------------------------------------------------------------------------- #
#   MySQL                                                                    #
# -------------------------------------------------------------------------- #

# MySQL host
#mysql_server:

# MySQL user. It runs "SHOW STATUS" and "SHOW SLAVE STATUS" queries.
# You should consider creating a separate user, e.g. datadog.
# CREATE USER 'datadog'@'localhost' identified by 'my_password';
# The latter requires the "REPLICATION CLIENT" privileges using a command like:
# GRANT REPLICATION CLIENT ON *.* TO 'datadog'@'localhost';
# http://dev.mysql.com/doc/refman/5.1/en/privileges-provided.html#priv_replication-client
#mysql_user:

# MySQL user's password
#mysql_pass:

# -------------------------------------------------------------------------- #
#   PostgreSQL                                                               #
# -------------------------------------------------------------------------- #

# PostgreSQL host
#postgresql_server:

# PostgreSQL port
#postgresql_port:

# PostgreSQL user. It needs to connect to the "postgres" database but does not
# require any privileges, so you should consider creating a separate,
# unprivileged user
#postgresql_user:

# PostgreSQL user's password
#postgresql_pass:


# -------------------------------------------------------------------------- #
#   Nginx                                                                    #
# -------------------------------------------------------------------------- #

# Url to nginx's status page. Must have http_stub_status_module installed.
# See http://wiki.nginx.org/HttpStubStatusModule for details.
#nginx_status_url: http://www.example.com/nginx_status


# -------------------------------------------------------------------------- #
#   RabbitMQ                                                                 #
# -------------------------------------------------------------------------- #

# Url to RabbitMQ's status page. Must have rabbitmq-status plugin installed.
# See http://www.lshift.net/blog/2009/11/30/introducing-rabbitmq-status-plugin
# for details.
#rabbitmq_status_url: http://www.example.com:55672/json
#rabbitmq_user: guest
#rabbitmq_pass: guest


# -------------------------------------------------------------------------- #
#   MongoDB                                                                  #
# -------------------------------------------------------------------------- #

# MongoDB uri. For example: mongodb://my_user:my_pass@localhost/my_db
#mongodb_server:

# -------------------------------------------------------------------------- #
#   CouchDB                                                                  #
# -------------------------------------------------------------------------- #

# CouchDB host
#couchdb_server:

# -------------------------------------------------------------------------- #
#   Cassandra                                                                #
# -------------------------------------------------------------------------- #
#cassandra_host: localhost
#cassandra_port: 8080
#cassandra_nodetool: /usr/bin/nodetool

# -------------------------------------------------------------------------- #
#   Hudson                                                                   #
# -------------------------------------------------------------------------- #

# Path to Hudson's home directory
# Make sure the dd-agent user can read this directory
#hudson_home: /var/lib/hudson/


# -------------------------------------------------------------------------- #
#   Nagios                                                                   #
# -------------------------------------------------------------------------- #

# dd-agent imports alerts and perfdata from nagios.

# Path to Nagios' event log file
# Make sure the dd-agent user can read this file
#nagios_log: /var/log/nagios3/nagios.log

# If you use perfdata, dd-agent can import automatically and in real-time
# performance data collected by nagios.
# For more information on perfdata configuration, please refer to
# http://nagios.sourceforge.net/docs/3_0/perfdata.html
#
# Path to Nagios' ***configuration*** file where the properties
# host|service_perfdata_file and host|service_perfdata_file_template
# are defined.
# (ubuntu 10.04)
#nagios_perf_cfg: /etc/nagios3/nagios.cfg
# (centos 5)
#nagios_perf_cfg: /etc/nagios/nagios.cfg


# -------------------------------------------------------------------------- #
#   Java                                                                     #
# -------------------------------------------------------------------------- #

# Get Java JVM metrics using JMX
# You can set multiple Java instances, using the schema below
# user is the JMX user (if any) used to connect to the related JMX instance
# password is the JMX password (if any) used to connect to the related JMX instance
# server is the JMX server url
# port is the port where the JMX server runs
# tag is the tag you want to associate to this instance, it's an optional parameter
# if not present the tag would be "server-port"


#java_jmx_instance_1: user:password@server:port:tag

#java_jmx_instance_2: user:password@server:port:tag

#...

# -------------------------------------------------------------------------- #
#   Tomcat                                                                   #
# -------------------------------------------------------------------------- #

# Get Tomcat metrics using JMX
# You can set multiple Tomcat instances, using the schema below
# user is the JMX user (if any) used to connect to the related JMX instance
# password is the JMX password (if any) used to connect to the related JMX instance
# server is the JMX server url
# port is the port where the JMX server runs
# tag is the tag you want to associate to this instance, it's an optional parameter
# if not present the tag would be "server-port"

#tomcat_jmx_instance_1: user:password@server:port:tag

#tomcat_jmx_instance_2: user:password@server:port:tag

#...



# -------------------------------------------------------------------------- #
#   Activemq                                                                 #
# -------------------------------------------------------------------------- #

# Get Activemq metrics using JMX
# You can set multiple ActiveMQ instances, using the schema below
# user is the JMX user (if any) used to connect to the related JMX instance
# password is the JMX password (if any) used to connect to the related JMX instance
# server is the JMX server url
# port is the port where the JMX server runs
# tag is the tag you want to associate to this instance, it's an optional parameter
# if not present the tag would be "server-port"


#activemq_jmx_instance_1: user:password@server:port:tag

#activemq_jmx_instance_2: user:password@server:port:tag

#...

# -------------------------------------------------------------------------- #
#  Solr                                                                      #
# -------------------------------------------------------------------------- #

# Get Solr metrics using JMX
# You can set multiple Solr instances, using the schema below
# user is the JMX user (if any) used to connect to the related JMX instance
# password is the JMX password (if any) used to connect to the related JMX instance
# server is the JMX server url
# port is the port where the JMX server runs
# tag is the tag you want to associate to this instance, it's an optional parameter
# if not present the tag would be "server-port"

#solr_jmx_instance_1: user:password@server:port:tag

#solr_jmx_instance_2: user:password@server:port:tag

#...

# -------------------------------------------------------------------------- #
#  Memcache
# -------------------------------------------------------------------------- #

#memcache_server: localhost
#memcache_port: 11211

# -------------------------------------------------------------------------- #
#  Dogstream (log file parser)
# -------------------------------------------------------------------------- #

# Comma-separated list of logs to parse and optionally custom parsers to use.
# The form should look like this:
#
#   dogstreams: /path/to/log1:parsers_module:custom_parser, /path/to/log2, /path/to/log3, ...
#
# Or this:
#
#   dogstreams: /path/to/log1:/path/to/my/parsers_module.py:custom_parser, /path/to/log2, /path/to/log3, ...
#
# Each entry is a path to a log file and optionally a Python module/function pair
# separated by colons.
#
# Custom parsers should take a 2 parameters, a logger object and
# a string parameter of the current line to parse. It should return a tuple of
# the form:
#   (metric (str), timestamp (unix timestamp), value (float), attributes (dict))
# where attributes should at least contain the key 'metric_type', specifying
# whether the given metric is a 'counter' or 'gauge'.
#
# Unless parsers are specified with an absolute path, the modules must exist in
# the agent's PYTHONPATH. You can set this as an environment variable when
# starting the agent. If the name of the custom parser function is not passed,
# 'parser' is assumed.
#
# If this value isn't specified, the default parser assumes this log format:
#     metric timestamp value key0=val0 key1=val1 ...
#

# -------------------------------------------------------------------------- #
#  Cacti
# -------------------------------------------------------------------------- #
#cacti_mysql_server: localhost
#cacti_mysql_user: dd-agent
#cacti_mysql_pass: agent
#cacti_rrd_path: /var/lib/cacti/rra

# If you want to whitelist only certain rrd files, you can add a list of patterns
# to a cacti rrd whitelist file, and reference it here.
#     e.g.: *localhost_load*.rrd
#cacti_rrd_whitelist: /etc/dd-agent/cacti-whitelist.txt

# -------------------------------------------------------------------------- #
#  Varnish
# -------------------------------------------------------------------------- #
#varnishstat: /usr/bin/varnishstat

# -------------------------------------------------------------------------- #
#  Redis
#
#   To monitor one or more redis instances, list their connection
#   information below:
# -------------------------------------------------------------------------- #
#
#redis_urls: localhost:6379, password@myserver:16379

# -------------------------------------------------------------------------- #
#  ElaticSearch
# -------------------------------------------------------------------------- #
#elasticsearch: http://localhost:9200/_cluster/nodes/stats?all=true

# -------------------------------------------------------------------------- #
#  HAProxy
# -------------------------------------------------------------------------- #
#haproxy_url: https://localhost/admin?stats

# If basic authentification is enabled
#haproxy_user = username
#haproxy_password = password

# ========================================================================== #
# Custom Emitters                                                            #
# ========================================================================== #

# Comma-separated list of emitters to be used in addition to the standard one
#
# Expected to be passed as a comma-separated list of colon-delimited
# name/object pairs.
#
# custom_emitters: /usr/local/my-code/emitters/rabbitmq.py:RabbitMQEmitter
#
# If the name of the emitter function is not specified, 'emitter' is assumed.


# ========================================================================== #
# Custom Checks
# ========================================================================== #

# Comma-separated list of additional metric checks
#
# Expected to be passed as a comma-separated list of colon-delimited
# name/object pairs.
#
# custom_checks: /usr/local/my-code/checks/foo.py:FooCheck
#
# If the name of the check is not specified, 'Check' is assumed.

##############################################################################
#################### NOTHING TO MODIFY AFTER THIS LINE #######################
##############################################################################

# -------------------------------------------------------------------------- #
#  Logging Configuration
#
#  PLEASE READ ME!
#
#  DO NOT PUT ANY INTEGRATION CONFIGURATION AFTER THIS SECTION
#  IT WILL BE IGNORED.
#  I REPEAT, IT WILL BE IGNORED.
#
# -------------------------------------------------------------------------- #

[loggers]
keys:root,dogstatsd,checks

[handlers]
keys:dogstatsd,ddagent

[formatters]
keys:ddagent

# DogStatsd logging

[logger_root]
level:ERROR
handlers:
propagate:0
qualname:root

[logger_dogstatsd]
level:ERROR
handlers:dogstatsd
propagate:0
qualname:dogstatsd

[logger_checks]
level:ERROR
handlers:ddagent
propogate:0
qualname:checks

[handler_dogstatsd]
class:handlers.NTEventLogHandler
formatter:ddagent
args=('DogstatsD', '', 'Application')

[handler_ddagent]
class:handlers.NTEventLogHandler
formatter:ddagent
args=('Datadog Agent', '', 'Application')

[handler_ddforwarder]
class:handlers.NTEventLogHandler
formatter:ddagent
args=('Datadog Forwarder', '', 'Application')

[formatter_ddagent]
format: %(asctime)s | %(name)s | %(levelname)s | %(message)s
class:logging.Formatter

# IF YOU ADD ANYTHING HERE, IT WILL BE IGNORED
# SO IF YOU WANT TO CONFIGURE AN INTEGRATION,
# DO IT IN THE SECTION ABOVE

